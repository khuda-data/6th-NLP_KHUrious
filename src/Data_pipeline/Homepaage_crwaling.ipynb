{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class ChemEngScraper:\n",
    "    def __init__(self, base_url):\n",
    "        \"\"\"초기화 및 기본 설정\"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_0_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.277 Whale/2.9.118.38 Safari/537.36'\n",
    "        }\n",
    "        self.hrefs = []\n",
    "        self.all_content_texts = []\n",
    "\n",
    "    def get_lnb_links(self):\n",
    "        \"\"\"웹 페이지에서 lnb 요소 내부의 depth 링크를 추출\"\"\"\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(self.base_url)\n",
    "        \n",
    "        # lnb 요소 내부의 class가 \"depth\"인 모든 a 태그에서 href 속성 추출\n",
    "        lnb_element = driver.find_element(\"xpath\", '//*[@id=\"lnb\"]')\n",
    "        depth_elements = lnb_element.find_elements(\"xpath\", './/*[@class=\"depth\"]//a')\n",
    "        self.hrefs = [element.get_attribute(\"href\") for element in depth_elements if element.get_attribute(\"href\")]\n",
    "        \n",
    "        driver.quit()\n",
    "    \n",
    "    def fetch_content_from_url(self, url):\n",
    "        \"\"\"URL에서 id='cont' 요소의 텍스트 콘텐츠를 가져옵니다\"\"\"\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve {url}, status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        # BeautifulSoup을 사용해 HTML 파싱\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content_div = soup.find(id=\"cont\")\n",
    "        \n",
    "        if content_div:\n",
    "            # 내부에서 class=\"hide\"인 요소 제거\n",
    "            for hidden in content_div.select(\".hide\"):\n",
    "                hidden.extract()\n",
    "\n",
    "            # 'inner_contents_title mt' 또는 'inner_contents_title mb' 요소의 앞뒤 \\n 개수 맞추기\n",
    "            self.adjust_special_elements(content_div)\n",
    "            \n",
    "            # 텍스트 내용에서 불필요한 줄바꿈과 공백 제거\n",
    "            content_text = self.clean_text(content_div.text)\n",
    "            return content_text\n",
    "        return None\n",
    "\n",
    "    def adjust_special_elements(self, content_div):\n",
    "        \"\"\"특정 클래스의 요소의 앞뒤 \\n 개수를 1로 맞춤\"\"\"\n",
    "        special_elements = content_div.find_all(class_=re.compile(r'inner_contents_title (mt|mb)'))\n",
    "        for special in special_elements:\n",
    "            if special.text:\n",
    "                text = special.text.strip()\n",
    "                # 앞뒤에 \\n이 정확히 1개씩 되도록 조정\n",
    "                if not text.startswith(\"\\n\"):\n",
    "                    text = \"\\n\" + text\n",
    "                if not text.endswith(\"\\n\"):\n",
    "                    text = text + \"\\n\"\n",
    "                special.string = text\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"텍스트에서 불필요한 줄바꿈과 공백을 정리\"\"\"\n",
    "        # \\n이 3개 이상 연속되면 2개로 줄이기\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n\\n', text)\n",
    "        # 캐리지 리턴 제거 및 여러 공백을 하나의 공백으로 줄이기\n",
    "        return text.replace('\\r', ' ').replace('  ', ' ').strip()\n",
    "\n",
    "    def scrape_all_content(self):\n",
    "        \"\"\"모든 URL을 순회하며 콘텐츠를 수집\"\"\"\n",
    "        self.get_lnb_links()\n",
    "        for href in self.hrefs:\n",
    "            content_text = self.fetch_content_from_url(href)\n",
    "            if content_text:\n",
    "                self.all_content_texts.append(content_text)\n",
    "    \n",
    "    def get_combined_content(self):\n",
    "        \"\"\"모든 콘텐츠를 하나의 텍스트로 결합\"\"\"\n",
    "        return \"\\n\\n\".join(self.all_content_texts)\n",
    "\n",
    "    def display_combined_content(self):\n",
    "        \"\"\"결합된 텍스트 콘텐츠를 출력\"\"\"\n",
    "        combined_content = self.get_combined_content()\n",
    "        # print(\"Combined Content from All URLs:\")\n",
    "        print(combined_content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")  # 구분선\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChemEngScraper 클래스를 임포트했다고 가정하고 사용\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://chemeng.khu.ac.kr/chemeng/user/main/view.do\"\n",
    "\n",
    "# ChemEngScraper 객체 생성 및 데이터 스크래핑 실행\n",
    "scraper = ChemEngScraper(base_url)\n",
    "\n",
    "# 모든 콘텐츠를 스크래핑\n",
    "scraper.scrape_all_content()\n",
    "\n",
    "# 결합된 콘텐츠 출력\n",
    "scraper.display_combined_content()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

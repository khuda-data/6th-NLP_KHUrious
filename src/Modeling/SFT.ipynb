{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433ddd1e8028bb07",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:36.486463Z",
     "start_time": "2024-11-25T12:13:32.181611Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Huggingface\n",
    "import huggingface_hub\n",
    "from transformers import TextStreamer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Unsloth\n",
    "from unsloth import FastLanguageModel, FastVisionModel, is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35dfb5dcffb629",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93281c231467f7e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:36.494554Z",
     "start_time": "2024-11-25T12:13:36.487953Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    debug: bool = False\n",
    "    \n",
    "    # Model\n",
    "    model_id: str = \"PathFinderKR/KHU-Llama-3.2-3B\"\n",
    "    model_type: str = \"language\"  # vision | language\n",
    "    \n",
    "    # HuggingFace Hub\n",
    "    username: str = \"PathFinderKR\"\n",
    "    model_name: str = f\"KHU-Llama-3.2-1B-Instruct-SFT\"\n",
    "    \n",
    "    # Data\n",
    "    dataset_id: str = \"mlabonne/FineTome-100k\"  # \"yahma/alpaca-cleaned\"\n",
    "    dataset_template: str = \"alpaca\"  # alpaca | chat\n",
    "    \n",
    "    # Training\n",
    "    ## Paths\n",
    "    output_dir: str = \"./results\"\n",
    "    logging_dir: str = \"./logs\"\n",
    "    save_strategy: str = \"epoch\"\n",
    "    logging_strategy: str = \"steps\"\n",
    "    logging_steps: int = 10\n",
    "    save_total_limit: int = 1\n",
    "    report_to: str = \"wandb\" if not debug else None\n",
    "    ## Hyperparameters\n",
    "    num_train_epochs: int = 1\n",
    "    per_device_train_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    fp16: bool = not is_bf16_supported()\n",
    "    bf16: bool = is_bf16_supported()\n",
    "    dtype: torch.dtype = torch.bfloat16 if is_bf16_supported() else torch.float16\n",
    "    load_in_4bit: bool = True\n",
    "    learning_rate: float = 2e-5\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    warmup_ratio: float = 0.1\n",
    "    optim: str = \"adamw_8bit\"\n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_length: int = 1024\n",
    "    dataset_num_proc: int = 2\n",
    "    packing: bool = True\n",
    "    ### LoRA\n",
    "    lora: bool = False\n",
    "    if lora:\n",
    "        r: int = 16\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"]\n",
    "        lora_alpha: int = 32\n",
    "        lora_dropout: float = 0\n",
    "        bias: str = \"none\"\n",
    "        use_gradient_checkpointing: str = \"unsloth\"\n",
    "        use_rslora: bool = False\n",
    "        loftq_config: str = None\n",
    "        save_method: str = \"merged_16bit\"\n",
    "    \n",
    "    # Inference\n",
    "    max_new_tokens: int = 1024\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    repetition_penalty: float = 1.1\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = None\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5525bda12104bb7a",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f89213fe1e3b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:36.505054Z",
     "start_time": "2024-11-25T12:13:36.495687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7844e5ceef3bb8a7",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24f8a736bfe545f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:36.514130Z",
     "start_time": "2024-11-25T12:13:36.506583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on GPU | Num of GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "def configure_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(\"> Running on GPU\", end=' | ')\n",
    "        print(\"Num of GPUs: \", num_gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"> Running on MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"> Running on CPU\")\n",
    "    return device\n",
    "\n",
    "CONFIG.device = configure_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fadfa0dd0dd4538",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214b07b072f88149",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:36.520929Z",
     "start_time": "2024-11-25T12:13:36.515261Z"
    }
   },
   "outputs": [],
   "source": [
    "if CONFIG.debug:\n",
    "    CONFIG.num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148b23c553f1ff3",
   "metadata": {},
   "source": [
    "## HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67bd0191d59a818f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:36.807315Z",
     "start_time": "2024-11-25T12:13:36.522061Z"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "huggingface_hub.login(\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "    add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1cd43b2bdb24d3",
   "metadata": {},
   "source": [
    "## Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c1d79090b69737f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:38.969213Z",
     "start_time": "2024-11-25T12:13:36.808430Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpathfinderkr\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/pathfinder/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/cheir/GitHub/KHU-Llama/wandb/run-20241125_211338-o3zfhtge</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-1B-Instruct-SFT/runs/o3zfhtge' target=\"_blank\">soft-glitter-1</a></strong> to <a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-1B-Instruct-SFT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-1B-Instruct-SFT' target=\"_blank\">https://wandb.ai/pathfinderkr/KHU-Llama-3.2-1B-Instruct-SFT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-1B-Instruct-SFT/runs/o3zfhtge' target=\"_blank\">https://wandb.ai/pathfinderkr/KHU-Llama-3.2-1B-Instruct-SFT/runs/o3zfhtge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not CONFIG.debug:\n",
    "    wandb.login(\n",
    "        key=os.getenv(\"WANDB_API_KEY\")\n",
    "    )\n",
    "    wandb.init(\n",
    "        project=CONFIG.model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295cb5ac174c88b5",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ff6c3caa4b6336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:38.975938Z",
     "start_time": "2024-11-25T12:13:38.970466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "llama_3_instruct_prompt = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "alpaca = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You're name is KHUrious. You are an expert in Kyung Hee University. Given a question from the user about Kyung Hee University, appropriately answer the question.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "def alpaca_template(examples):\n",
    "    texts = []\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        text = tokenizer.bos_token + alpaca.format(instruction, output) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Formatting functions\n",
    "def apply_alpaca_template(examples):\n",
    "    texts = []\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = tokenizer.bos_token + alpaca_prompt.format(instruction, input, output) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "def apply_llama_template(examples):\n",
    "    texts = []\n",
    "    for conversation in examples:\n",
    "        system = \"\"\n",
    "        user = \"\"\n",
    "        assistant = \"\"\n",
    "        for message in conversation:\n",
    "            if message['from'] == 'system':\n",
    "                system = message['value']\n",
    "            elif message['from'] == 'human':\n",
    "                user = message['value']\n",
    "            elif message['from'] == 'gpt':\n",
    "                assistant = message['value']\n",
    "        if CONFIG.model_type == \"language\":\n",
    "            text = tokenizer.bos_token + llama_3_instruct_prompt.format(system, user, assistant) + tokenizer.eos_token\n",
    "        elif CONFIG.model_type == \"vision\":\n",
    "            text = processor.bos_token + llama_3_instruct_prompt.format(system, user, assistant) + processor.eos_token\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type\")\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9367124b154dabf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:38.985629Z",
     "start_time": "2024-11-25T12:13:38.977230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate base model\n",
    "def generate_text(prompt):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prompt\n",
    "    ], return_tensors = \"pt\").to(CONFIG.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "# Generate instruction model\n",
    "def generate_response(system, user):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        llama_3_instruct_prompt.format(\n",
    "            system,\n",
    "            user,\n",
    "            \"\"\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(CONFIG.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "# Generate vision model\n",
    "def generate_vision(system, user):\n",
    "    FastVisionModel.for_inference(model)\n",
    "    input_text = [\n",
    "        llama_3_instruct_prompt.format(\n",
    "            system,\n",
    "            user,\n",
    "            \"\"\n",
    "        )\n",
    "    ]\n",
    "    inputs = processor(\n",
    "        images=None,\n",
    "        texts=input_text,\n",
    "        return_tensors = \"pt\"\n",
    "    ).to(CONFIG.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da41a5bf2f6a36f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:38.995098Z",
     "start_time": "2024-11-25T12:13:38.987751Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_token_length(fields):\n",
    "    for field in fields:\n",
    "        token_lengths = [len(tokenizer.encode(example[field])) for example in dataset if example[field] != \"\"]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(token_lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel(f'{field.capitalize()} Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{field.capitalize()} Token Length Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Max {field} token length: {max(token_lengths)}\")\n",
    "        print(f\"Min {field} token length: {min(token_lengths)}\")\n",
    "        print(f\"Mean {field} token length: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"Standard deviation of {field} token length: {np.std(token_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82f87f74b3d94e9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb1e60af7efcb450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:48.532352Z",
     "start_time": "2024-11-25T12:13:38.996416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.8: Fast Llama patching. Transformers = 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4080 SUPER. Max memory: 15.992 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.model_type == \"language\":\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=CONFIG.model_id,\n",
    "        max_seq_length=CONFIG.max_seq_length,\n",
    "        dtype=CONFIG.dtype,\n",
    "        load_in_4bit=CONFIG.load_in_4bit if CONFIG.lora else False\n",
    "    )\n",
    "elif CONFIG.model_type == \"vision\":\n",
    "    model, processor = FastVisionModel.from_pretrained(\n",
    "        model_name=CONFIG.model_id,\n",
    "        max_seq_length=CONFIG.max_seq_length,\n",
    "        dtype=CONFIG.dtype,\n",
    "        load_in_4bit=CONFIG.load_in_4bit if CONFIG.lora else False\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Invalid model type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2630dd4cb3663ac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:48.536819Z",
     "start_time": "2024-11-25T12:13:48.533433Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "#print(f\"Special tokens: {tokenizer.all_special_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7176bb1ccdeeca4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:48.547541Z",
     "start_time": "2024-11-25T12:13:48.538099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "Number of parameters: 1.24B\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_parameters() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa3bb022dea115b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:48.553517Z",
     "start_time": "2024-11-25T12:13:48.548743Z"
    }
   },
   "outputs": [],
   "source": [
    "if CONFIG.debug:\n",
    "    sample_system = \"You are a helpful assistant.\"\n",
    "    sample_user = \"What is the capital of France?\"\n",
    "    if CONFIG.model_type == \"language\":\n",
    "        sample_response = generate_response(sample_system, sample_user)\n",
    "        print(sample_response)\n",
    "        #print(tokenizer.tokenize(sample_response[0]))\n",
    "    elif CONFIG.model_type == \"vision\":\n",
    "        sample_response = generate_vision(sample_system, sample_user)\n",
    "        print(sample_response)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632317c22989499",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6f8bf4732ed38ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:48.563224Z",
     "start_time": "2024-11-25T12:13:48.554762Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataset = load_dataset(CONFIG.dataset_id, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "163266298433ee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:49.177788Z",
     "start_time": "2024-11-25T12:13:48.564615Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files='KUHrious_SFT_Dataset.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63f155605ee4c12c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:49.183035Z",
     "start_time": "2024-11-25T12:13:49.178826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 6993\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50e594b242b76d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:49.190165Z",
     "start_time": "2024-11-25T12:13:49.184275Z"
    }
   },
   "outputs": [],
   "source": [
    "if CONFIG.debug:\n",
    "    if CONFIG.dataset_template == \"alpaca\":\n",
    "        print(f\"instruction: {dataset[0]['instruction']}\")\n",
    "        print(f\"input: {dataset[0]['input']}\")\n",
    "        print(f\"output: {dataset[0]['output']}\")\n",
    "    elif CONFIG.dataset_template == \"chat\":\n",
    "        #print(f\"conversations: {dataset[0]['features']}\")\n",
    "        #print(f\"source: {dataset[0]['source']}\")\n",
    "        #print(f\"score: {dataset[0]['score']}\")\n",
    "        print(dataset[0])\n",
    "        print(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333975d5c50820a5",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "898878d6328ab66b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:49.314296Z",
     "start_time": "2024-11-25T12:13:49.191181Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(alpaca_template, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb35ca6533c912b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:49.319740Z",
     "start_time": "2024-11-25T12:13:49.315325Z"
    }
   },
   "outputs": [],
   "source": [
    "if CONFIG.debug:\n",
    "    print(dataset[0][\"text\"])\n",
    "    #print(tokenizer.tokenize(dataset[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2569cf935d78df9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:49.326624Z",
     "start_time": "2024-11-25T12:13:49.320796Z"
    }
   },
   "outputs": [],
   "source": [
    "if CONFIG.debug:\n",
    "    plot_token_length([\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9029bed5c49bee3",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41dbb68405aa52b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:49.333566Z",
     "start_time": "2024-11-25T12:13:49.327681Z"
    }
   },
   "outputs": [],
   "source": [
    "if CONFIG.lora:\n",
    "    if CONFIG.model_type == \"language\":\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=CONFIG.r,\n",
    "            target_modules=CONFIG.target_modules,\n",
    "            lora_alpha=CONFIG.lora_alpha,\n",
    "            lora_dropout=CONFIG.lora_dropout,\n",
    "            bias=CONFIG.bias,\n",
    "            use_gradient_checkpointing=CONFIG.use_gradient_checkpointing,\n",
    "            use_rslora=CONFIG.use_rslora,\n",
    "            loftq_config=CONFIG.loftq_config,\n",
    "            random_state=CONFIG.seed\n",
    "        )\n",
    "    elif CONFIG.model_type == \"vision\":\n",
    "        model = FastVisionModel.get_peft_model(\n",
    "            model,\n",
    "            finetune_vision_layers     = False, # False if not finetuning vision layers\n",
    "            finetune_language_layers   = True, # False if not finetuning language layers\n",
    "            finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "            finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "            \n",
    "            r=CONFIG.r,\n",
    "            target_modules=CONFIG.target_modules,\n",
    "            lora_alpha=CONFIG.lora_alpha,\n",
    "            lora_dropout=CONFIG.lora_dropout,\n",
    "            bias=CONFIG.bias,\n",
    "            use_gradient_checkpointing=CONFIG.use_gradient_checkpointing,\n",
    "            use_rslora=CONFIG.use_rslora,\n",
    "            loftq_config=CONFIG.loftq_config,\n",
    "            random_state=CONFIG.seed\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f005f3823824359e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:49.342130Z",
     "start_time": "2024-11-25T12:13:49.334693Z"
    }
   },
   "outputs": [],
   "source": [
    "if CONFIG.lora:\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2d9843552b6ff26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:13:49.497707Z",
     "start_time": "2024-11-25T12:13:49.343219Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=CONFIG.max_seq_length,\n",
    "    dataset_num_proc=CONFIG.dataset_num_proc,\n",
    "    packing=CONFIG.packing,\n",
    "    data_collator=UnslothVisionDataCollator(model, tokenizer) if CONFIG.model_type == \"vision\" else None,\n",
    "    args=SFTConfig(\n",
    "        output_dir=CONFIG.output_dir,\n",
    "        logging_dir=CONFIG.logging_dir,\n",
    "        save_strategy=CONFIG.save_strategy,\n",
    "        logging_strategy=CONFIG.logging_strategy,\n",
    "        logging_steps=CONFIG.logging_steps,\n",
    "        save_total_limit=CONFIG.save_total_limit,\n",
    "        report_to=CONFIG.report_to,\n",
    "        num_train_epochs=CONFIG.num_train_epochs,\n",
    "        per_device_train_batch_size=CONFIG.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=CONFIG.gradient_accumulation_steps,\n",
    "        fp16=CONFIG.fp16,\n",
    "        bf16=CONFIG.bf16,\n",
    "        learning_rate=CONFIG.learning_rate,\n",
    "        lr_scheduler_type=CONFIG.lr_scheduler_type,\n",
    "        warmup_ratio=CONFIG.warmup_ratio,\n",
    "        optim=CONFIG.optim,\n",
    "        weight_decay=CONFIG.weight_decay\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fef742b59ae750cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:19:45.297377Z",
     "start_time": "2024-11-25T12:13:49.498692Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,846 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 1 | Total steps = 1,846\n",
      " \"-____-\"     Number of trainable parameters = 973,146,112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1846' max='1846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1846/1846 05:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.929900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.855300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.788100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.602200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.564900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.533700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.454600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.395900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.303200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.266900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.376200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.328400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.424500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.169600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.314200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.229100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.249200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.194500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.259700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.186200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.274600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.189500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.164200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>1.199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>1.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>1.239100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>1.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.144200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.189500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>1.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.161600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>1.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>1.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>1.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>1.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>1.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.169600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>1.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>1.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>1.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>1.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>1.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>1.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>1.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>1.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>1.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>1.098900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.083500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>1.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>1.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.187400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1846, training_loss=1.2524690320664857, metrics={'train_runtime': 354.2586, 'train_samples_per_second': 5.211, 'train_steps_per_second': 5.211, 'total_flos': 1.1037251928588288e+16, 'train_loss': 1.2524690320664857, 'epoch': 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81f595ecc945c59b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:20:01.875016Z",
     "start_time": "2024-11-25T12:19:45.299020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b344b1c955141948e994fb36b413668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>██▃▃▂▃▃▂▁▂▂▂▁▂▁▂▁▁▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂</td></tr><tr><td>train/learning_rate</td><td>▃▃▅▇███████▇▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▄▂▂▂▃▂▂▂▁▂▂▂▂▂▁▂▁▂▁▁▁▂▁▁▂▁▁▂▂▂▁▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.1037251928588288e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>1846</td></tr><tr><td>train/grad_norm</td><td>7.96875</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1874</td></tr><tr><td>train_loss</td><td>1.25247</td></tr><tr><td>train_runtime</td><td>354.2586</td></tr><tr><td>train_samples_per_second</td><td>5.211</td></tr><tr><td>train_steps_per_second</td><td>5.211</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-glitter-1</strong> at: <a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-1B-Instruct-SFT/runs/o3zfhtge' target=\"_blank\">https://wandb.ai/pathfinderkr/KHU-Llama-3.2-1B-Instruct-SFT/runs/o3zfhtge</a><br/> View project at: <a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-1B-Instruct-SFT' target=\"_blank\">https://wandb.ai/pathfinderkr/KHU-Llama-3.2-1B-Instruct-SFT</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241125_211338-o3zfhtge/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not CONFIG.debug:\n",
    "    wandb.finish()\n",
    "    if CONFIG.lora:\n",
    "        model.save_pretrained(CONFIG.model_name + \"-LoRA\")\n",
    "        tokenizer.save_pretrained(CONFIG.model_name + \"-LoRA\")\n",
    "    else:\n",
    "        model.save_pretrained(CONFIG.model_name)\n",
    "        tokenizer.save_pretrained(CONFIG.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc305346d4050e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7b3749dec022836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:20:01.879801Z",
     "start_time": "2024-11-25T12:20:01.876740Z"
    }
   },
   "outputs": [],
   "source": [
    "if CONFIG.lora:\n",
    "    sample_system = \"You are a helpful assistant.\"\n",
    "    sample_user = \"What is the capital of France?\"\n",
    "    if CONFIG.model_type == \"language\":\n",
    "        sample_response = generate_response(sample_system, sample_user)\n",
    "    elif CONFIG.model_type == \"vision\":\n",
    "        sample_response = generate_vision(sample_system, sample_user)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "    print(sample_response)\n",
    "    print(tokenizer.tokenize(sample_response[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cf1b4ed3708c3",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4c00c867c11af7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:24:49.777823Z",
     "start_time": "2024-11-25T12:20:01.882328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec56c0cee1944d98bcf80f804ef16532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965fabaed3984852a75e4b1cc224dfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/PathFinderKR/KHU-Llama-3.2-1B-Instruct-SFT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f9be3744044c678fc561ee72a3f8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/589 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315cf029cabf4d30a2d75c8f8510e8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not CONFIG.debug:\n",
    "    if CONFIG.lora:\n",
    "        model.save_pretrained_merged(\n",
    "            CONFIG.model_name,\n",
    "            tokenizer,\n",
    "            save_method=CONFIG.save_method\n",
    "        )\n",
    "        model.push_to_hub_merged(\n",
    "            CONFIG.model_name,\n",
    "            tokenizer,\n",
    "            save_method=CONFIG.save_method\n",
    "        )\n",
    "    else:\n",
    "        model.push_to_hub(\n",
    "            repo_id=CONFIG.username + \"/\" + CONFIG.model_name,\n",
    "            use_temp_dir=False\n",
    "        )\n",
    "        tokenizer.push_to_hub(\n",
    "            repo_id=CONFIG.username + \"/\" + CONFIG.model_name,\n",
    "            use_temp_dir=False\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

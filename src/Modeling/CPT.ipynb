{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "id": "433ddd1e8028bb07"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:21.115696Z",
     "start_time": "2024-11-21T14:32:16.703509Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Huggingface\n",
    "import huggingface_hub\n",
    "from transformers import TextStreamer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Unsloth\n",
    "from unsloth import FastLanguageModel, is_bf16_supported, UnslothTrainer, UnslothTrainingArguments"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "d35dfb5dcffb629"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:21.121774Z",
     "start_time": "2024-11-21T14:32:21.116905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    debug: bool = False\n",
    "    \n",
    "    # Model\n",
    "    model_id: str = \"meta-llama/Llama-3.2-3B\"\n",
    "    \n",
    "    # HuggingFace Hub\n",
    "    username: str = \"PathFinderKR\"\n",
    "    model_name: str = f\"KHU-Llama-3.2-3B\"\n",
    "    \n",
    "    # Data\n",
    "    dataset_id: str = \"Khudanlp/KHUrious_pretraining_data\"\n",
    "    \n",
    "    # Training\n",
    "    ## Paths\n",
    "    output_dir: str = \"./results\"\n",
    "    logging_dir: str = \"./logs\"\n",
    "    save_strategy: str = \"epoch\"\n",
    "    logging_strategy: str = \"steps\"\n",
    "    logging_steps: int = 1\n",
    "    save_total_limit: int = 1\n",
    "    report_to: str = \"wandb\" if not debug else None\n",
    "    ## Hyperparameters\n",
    "    num_train_epochs: int = 2\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    fp16: bool = not is_bf16_supported()\n",
    "    bf16: bool = is_bf16_supported()\n",
    "    dtype: torch.dtype = torch.bfloat16 if is_bf16_supported() else torch.float16\n",
    "    load_in_4bit: bool = True\n",
    "    learning_rate: float = 5e-4\n",
    "    embedding_learning_rate = 1e-4\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    warmup_ratio: float = 0.1\n",
    "    optim: str = \"adamw_8bit\"\n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_length: int = 12800\n",
    "    dataset_num_proc: int = 2\n",
    "    packing: bool = True\n",
    "    ### LoRA\n",
    "    lora: bool = True\n",
    "    if lora:\n",
    "        r: int = 128\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\"]\n",
    "        lora_alpha: int = 32\n",
    "        lora_dropout: float = 0\n",
    "        bias: str = \"none\"\n",
    "        use_gradient_checkpointing: str = \"unsloth\"\n",
    "        use_rslora: bool = True\n",
    "        loftq_config: str = None\n",
    "        save_method: str = \"merged_16bit\"\n",
    "    \n",
    "    # Inference\n",
    "    max_new_tokens: int = 2048\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    repetition_penalty: float = 1.1\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = None\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 42"
   ],
   "id": "93281c231467f7e7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "5525bda12104bb7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:21.131853Z",
     "start_time": "2024-11-21T14:32:21.122852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ],
   "id": "c7f89213fe1e3b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "7844e5ceef3bb8a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:21.140914Z",
     "start_time": "2024-11-21T14:32:21.133597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configure_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(\"> Running on GPU\", end=' | ')\n",
    "        print(\"Num of GPUs: \", num_gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"> Running on MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"> Running on CPU\")\n",
    "    return device\n",
    "\n",
    "CONFIG.device = configure_device()"
   ],
   "id": "b24f8a736bfe545f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on GPU | Num of GPUs:  1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Debugging",
   "id": "8fadfa0dd0dd4538"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:21.147536Z",
     "start_time": "2024-11-21T14:32:21.141983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    CONFIG.num_train_epochs = 1"
   ],
   "id": "214b07b072f88149",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HuggingFace",
   "id": "c148b23c553f1ff3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:21.411927Z",
     "start_time": "2024-11-21T14:32:21.148535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "huggingface_hub.login(\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "    add_to_git_credential=True\n",
    ")"
   ],
   "id": "67bd0191d59a818f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Weights & Biases",
   "id": "bd1cd43b2bdb24d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:23.476809Z",
     "start_time": "2024-11-21T14:32:21.413054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    wandb.login(\n",
    "        key=os.getenv(\"WANDB_API_KEY\")\n",
    "    )\n",
    "    wandb.init(\n",
    "        project=CONFIG.model_name\n",
    "    )"
   ],
   "id": "9c1d79090b69737f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/pathfinder/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/cheir/GitHub/KHU-Llama/wandb/run-20241121_233222-edjy3wxy</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-3B/runs/edjy3wxy' target=\"_blank\">glad-rain-1</a></strong> to <a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-3B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-3B' target=\"_blank\">https://wandb.ai/pathfinderkr/KHU-Llama-3.2-3B</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-3B/runs/edjy3wxy' target=\"_blank\">https://wandb.ai/pathfinderkr/KHU-Llama-3.2-3B/runs/edjy3wxy</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utility Functions",
   "id": "295cb5ac174c88b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:23.482015Z",
     "start_time": "2024-11-21T14:32:23.477926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate base model\n",
    "def generate_text(prompt):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prompt\n",
    "    ], return_tensors = \"pt\").to(CONFIG.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)"
   ],
   "id": "9367124b154dabf9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:23.490007Z",
     "start_time": "2024-11-21T14:32:23.483125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "khu_prompt = \"\"\"Kyung Hee University\n",
    "### Title: {}\n",
    "\n",
    "### Information:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "def formatting_func(examples):\n",
    "    texts = []\n",
    "    metadata = examples[\"meta\"]\n",
    "    data = examples[\"TEXT\"]\n",
    "    for metadata, data in zip(metadata, data):\n",
    "        text = khu_prompt.format(metadata, data) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ],
   "id": "e8cbded2937589b9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:23.498826Z",
     "start_time": "2024-11-21T14:32:23.491884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_token_length(fields):\n",
    "    for field in fields:\n",
    "        token_lengths = [len(tokenizer.encode(example[field])) for example in dataset if example[field] != \"\"]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(token_lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel(f'{field.capitalize()} Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{field.capitalize()} Token Length Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Max {field} token length: {max(token_lengths)}\")\n",
    "        print(f\"Min {field} token length: {min(token_lengths)}\")\n",
    "        print(f\"Mean {field} token length: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"Standard deviation of {field} token length: {np.std(token_lengths):.2f}\")"
   ],
   "id": "da41a5bf2f6a36f0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "d82f87f74b3d94e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:33.734335Z",
     "start_time": "2024-11-21T14:32:23.499928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG.model_id,\n",
    "    max_seq_length=CONFIG.max_seq_length,\n",
    "    dtype=CONFIG.dtype,\n",
    "    load_in_4bit=CONFIG.load_in_4bit if CONFIG.lora else False\n",
    ")"
   ],
   "id": "cb1e60af7efcb450",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.8: Fast Llama patching. Transformers = 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4080 SUPER. Max memory: 15.992 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:33.739103Z",
     "start_time": "2024-11-21T14:32:33.735629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.all_special_tokens}\")"
   ],
   "id": "2630dd4cb3663ac5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 128000\n",
      "Special tokens: ['<|begin_of_text|>', '<|end_of_text|>', '<|finetune_right_pad_id|>']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:33.749331Z",
     "start_time": "2024-11-21T14:32:33.740098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_parameters() / 1e9:.2f}B\")"
   ],
   "id": "e7176bb1ccdeeca4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ")\n",
      "Number of parameters: 3.21B\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:33.756119Z",
     "start_time": "2024-11-21T14:32:33.750362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    sample_text = \"\"\"Kyung Hee University\n",
    "### Title: Department of Computer Science and Engineering\n",
    "\n",
    "### Information:\n",
    "\"\"\"\n",
    "    sample_response = generate_text(sample_text)\n",
    "    print(sample_response)"
   ],
   "id": "aa3bb022dea115b9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "7632317c22989499"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:36.982264Z",
     "start_time": "2024-11-21T14:32:33.757158Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(CONFIG.dataset_id, split=\"train\")",
   "id": "f6f8bf4732ed38ab",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:36.987009Z",
     "start_time": "2024-11-21T14:32:36.983399Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "63f155605ee4c12c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['TEXT', 'meta'],\n",
       "    num_rows: 1690\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:36.993656Z",
     "start_time": "2024-11-21T14:32:36.987969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    print(dataset[0][\"TEXT\"])"
   ],
   "id": "50e594b242b76d34",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "333975d5c50820a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:37.126357Z",
     "start_time": "2024-11-21T14:32:36.994867Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = dataset.map(formatting_func, batched=True)",
   "id": "898878d6328ab66b",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:37.132138Z",
     "start_time": "2024-11-21T14:32:37.127216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    print(dataset[0][\"text\"])\n",
    "    print(tokenizer.tokenize(dataset[0][\"text\"]))"
   ],
   "id": "eb35ca6533c912b6",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:37.142229Z",
     "start_time": "2024-11-21T14:32:37.133253Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "16ad8c8e63f93496",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['TEXT', 'meta', 'text'],\n",
       "    num_rows: 1690\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:37.148418Z",
     "start_time": "2024-11-21T14:32:37.143329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    plot_token_length([\"text\"])"
   ],
   "id": "e2569cf935d78df9",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Continued Pre-Training (LoRA)",
   "id": "a9029bed5c49bee3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:58.442032Z",
     "start_time": "2024-11-21T14:32:37.149620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.lora:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=CONFIG.r,\n",
    "        target_modules=CONFIG.target_modules,\n",
    "        lora_alpha=CONFIG.lora_alpha,\n",
    "        lora_dropout=CONFIG.lora_dropout,\n",
    "        bias=CONFIG.bias,\n",
    "        use_gradient_checkpointing=CONFIG.use_gradient_checkpointing,\n",
    "        use_rslora=CONFIG.use_rslora,\n",
    "        loftq_config=CONFIG.loftq_config,\n",
    "        random_state=CONFIG.seed\n",
    "    )"
   ],
   "id": "41dbb68405aa52b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pathfinder/anaconda3/envs/torch-env/lib/python3.12/site-packages/unsloth/models/_utils.py:746: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:58.451653Z",
     "start_time": "2024-11-21T14:32:58.443644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.lora:\n",
    "    model.print_trainable_parameters()"
   ],
   "id": "f005f3823824359e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 982,515,712 || all params: 4,589,267,968 || trainable%: 21.4090\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:32:58.627610Z",
     "start_time": "2024-11-21T14:32:58.453022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=CONFIG.max_seq_length,\n",
    "    dataset_num_proc=CONFIG.dataset_num_proc,\n",
    "    packing=CONFIG.packing,\n",
    "    args=UnslothTrainingArguments(\n",
    "        output_dir=CONFIG.output_dir,\n",
    "        logging_dir=CONFIG.logging_dir,\n",
    "        save_strategy=CONFIG.save_strategy,\n",
    "        logging_strategy=CONFIG.logging_strategy,\n",
    "        logging_steps=CONFIG.logging_steps,\n",
    "        save_total_limit=CONFIG.save_total_limit,\n",
    "        report_to=CONFIG.report_to,\n",
    "        num_train_epochs=CONFIG.num_train_epochs,\n",
    "        per_device_train_batch_size=CONFIG.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=CONFIG.gradient_accumulation_steps,\n",
    "        fp16=CONFIG.fp16,\n",
    "        bf16=CONFIG.bf16,\n",
    "        learning_rate=CONFIG.learning_rate,\n",
    "        embedding_learning_rate=CONFIG.embedding_learning_rate,\n",
    "        lr_scheduler_type=CONFIG.lr_scheduler_type,\n",
    "        warmup_ratio=CONFIG.warmup_ratio,\n",
    "        optim=CONFIG.optim,\n",
    "        weight_decay=CONFIG.weight_decay\n",
    "    )\n",
    ")"
   ],
   "id": "d2d9843552b6ff26",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:49:46.873744Z",
     "start_time": "2024-11-21T14:32:58.628986Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "fef742b59ae750cf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 93 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 4 | Total steps = 46\n",
      " \"-____-\"     Number of trainable parameters = 982,515,712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 1.00e-04 instead of 5.00e-04 for embed_tokens.\n",
      "Unsloth: Setting lr = 1.00e-04 instead of 5.00e-04 for lm_head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 16:21, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.453700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.884700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.766700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.668200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.906400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.906500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.794800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.768800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.761500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.777600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.776600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.703700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.602100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.573100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.484900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.526300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.321800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.260300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=46, training_loss=1.5811909437179565, metrics={'train_runtime': 1006.0047, 'train_samples_per_second': 0.185, 'train_steps_per_second': 0.046, 'total_flos': 5.34244721688576e+16, 'train_loss': 1.5811909437179565, 'epoch': 1.9574468085106385})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:50:06.618297Z",
     "start_time": "2024-11-21T14:49:46.897913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    wandb.finish()\n",
    "    if CONFIG.lora:\n",
    "        model.save_pretrained(CONFIG.model_name + \"-LoRA\")\n",
    "        tokenizer.save_pretrained(CONFIG.model_name + \"-LoRA\")\n",
    "    else:\n",
    "        model.save_pretrained(CONFIG.model_name)\n",
    "        tokenizer.save_pretrained(CONFIG.model_name)"
   ],
   "id": "81f595ecc945c59b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.019 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbdaef17b8d740339c3e9500c8901ec5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▅█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▂▄▅▇███████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>██▆▆▅▄▆▅▅▄▄▄▄▄▄▄▃▄▃▃▆▁▃▂▂▂▂▃▂▁▁▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>5.34244721688576e+16</td></tr><tr><td>train/epoch</td><td>1.95745</td></tr><tr><td>train/global_step</td><td>46</td></tr><tr><td>train/grad_norm</td><td>0.37885</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.2603</td></tr><tr><td>train_loss</td><td>1.58119</td></tr><tr><td>train_runtime</td><td>1006.0047</td></tr><tr><td>train_samples_per_second</td><td>0.185</td></tr><tr><td>train_steps_per_second</td><td>0.046</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glad-rain-1</strong> at: <a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-3B/runs/edjy3wxy' target=\"_blank\">https://wandb.ai/pathfinderkr/KHU-Llama-3.2-3B/runs/edjy3wxy</a><br/> View project at: <a href='https://wandb.ai/pathfinderkr/KHU-Llama-3.2-3B' target=\"_blank\">https://wandb.ai/pathfinderkr/KHU-Llama-3.2-3B</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_233222-edjy3wxy/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "e912d53f6aa9676e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:50:23.203795Z",
     "start_time": "2024-11-21T14:50:06.620270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_text = \"\"\"Kyung Hee University\n",
    "### Title: Department of Computer Science and Engineering\n",
    "\n",
    "### Information:\n",
    "\"\"\"\n",
    "sample_response = generate_text(sample_text)"
   ],
   "id": "2fc3203546fc7644",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Kyung Hee University\n",
      "### Title: Department of Computer Science and Engineering\n",
      "\n",
      "### Information:\n",
      "The document outlines the curriculum for the undergraduate program in computer engineering at Kyung Hee University, known as the Software Convergence Department. It includes a detailed description of the educational objectives, the structure of the curriculum, and specific courses offered.The first section introduces the department, stating that it is part of the College of Electronic Information but operates independently under the name \"Software Convergence.\" The second section details the educational goals. These include nurturing creative talents who can lead future industries, fostering scientific thinking through interdisciplinary learning, and providing practical training to meet industry needs. To achieve these goals, the department aims to enhance students' problem-solving skills by combining theory with practice, emphasize communication and teamwork, and provide opportunities for international exchanges.The third section focuses on the curriculum structure, starting with the basic structure of the liberal arts education, which emphasizes critical knowledge acquisition and global perspectives. This is followed by a specialized track for each major, including computer engineering. For computer engineering, there are two tracks: an AI-focused track and a general track. Each track has its own requirements, such as completing various subjects across different departments.The fourth section presents the list of courses available within the software convergence department. Starting from the liberal arts, students must complete foundational mathematics, physics, chemistry, biology, and earth science subjects. They also have the option to take humanities or social sciences courses, depending on their preferences. The specialized tracks require additional courses, including programming fundamentals, data structures, algorithms, object-oriented programming, and operating systems. The AI-focused track offers more advanced subjects like artificial intelligence, machine learning, natural language processing, and robotics.The final sections outline other required subjects, such as coding practices and capstone design projects, and provide guidance on how to fulfill graduation requirements.Overall, this comprehensive overview provides an excellent foundation for anyone interested in pursuing a career in computer engineering or exploring the intersection of technology and art through interdisciplinary study.\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save",
   "id": "9b5cf1b4ed3708c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:13.157164Z",
     "start_time": "2024-11-21T14:50:23.224746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    if CONFIG.lora:\n",
    "        model.save_pretrained_merged(\n",
    "            CONFIG.model_name,\n",
    "            tokenizer,\n",
    "            save_method=CONFIG.save_method\n",
    "        )\n",
    "        model.push_to_hub_merged(\n",
    "            CONFIG.model_name,\n",
    "            tokenizer,\n",
    "            save_method=CONFIG.save_method\n",
    "        )\n",
    "    else:\n",
    "        model.push_to_hub(\n",
    "            repo_id=CONFIG.username + \"/\" + CONFIG.model_name,\n",
    "            use_temp_dir=False\n",
    "        )\n",
    "        tokenizer.push_to_hub(\n",
    "            repo_id=CONFIG.username + \"/\" + CONFIG.model_name,\n",
    "            use_temp_dir=False\n",
    "        )"
   ],
   "id": "a4c00c867c11af7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 0.0 out of 15.48 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 24/28 [00:00<00:00, 44.53it/s]We will save to Disk and not RAM now.\n",
      "100%|██████████| 28/28 [00:02<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 0.0 out of 15.48 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving to organization with address PathFinderKR/KHU-Llama-3.2-3B\n",
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Unsloth: Saving to organization with address PathFinderKR/KHU-Llama-3.2-3B\n",
      "Unsloth: Uploading all files... Please wait...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.25G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2eaea1e21e454cd88461555482ac3a6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47e538f0c5a94c1293c53f62d992bfbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efb977801a1e48f7aff139d2d35624b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cefddbc692fe485a867a09f54d5672af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/None/KHU-Llama-3.2-3B\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
